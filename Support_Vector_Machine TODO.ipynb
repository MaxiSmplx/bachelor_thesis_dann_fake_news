{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, joblib, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, GridSearchCV\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"prepare_datasets/ISO Fake News\")\n",
    "\n",
    "real_df = pd.read_csv(DATA_DIR / \"real.csv\")\n",
    "fake_df = pd.read_csv(DATA_DIR / \"fake.csv\")\n",
    "\n",
    "TEXT_COL = \"full_text\"\n",
    "real_df[TEXT_COL] = real_df[\"title\"] + real_df[\"text\"]\n",
    "real_df.drop([\"title\", \"text\"], axis=1, inplace=True)\n",
    "fake_df[TEXT_COL] = fake_df[\"title\"] + fake_df[\"text\"]\n",
    "fake_df.drop([\"title\", \"text\"], axis=1, inplace=True)\n",
    "\n",
    "real_df[\"label\"] = \"real\"\n",
    "fake_df[\"label\"] = \"fake\"\n",
    "\n",
    "df = pd.concat([real_df[[TEXT_COL, \"label\"]],\n",
    "                fake_df[[TEXT_COL, \"label\"]]],\n",
    "               ignore_index=True).sample(frac=1, random_state=42\n",
    ") \n",
    "\n",
    "X_raw, y = df[TEXT_COL], df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maxis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maxis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maxis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\maxis\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# INFO: Ran once to download stop words\n",
    "#import nltk\n",
    "#for pkg in (\"stopwords\", \"punkt\", \"wordnet\", \"omw-1.4\"):\n",
    "    #nltk.download(pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "URL_RE  = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "TAG_RE  = re.compile(r\"[@#]\\w+\")\n",
    "PUNCTUATION_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = TAG_RE.sub(\" \", text)\n",
    "    text = text.translate(PUNCTUATION_TABLE)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in text.lower().split()\n",
    "        if tok not in stops and len(tok) > 2\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X = X_raw.astype(str).apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maxis\\Desktop\\Bachelor Arbeit\\bachelor_thesis_dann_fake_news\\Support_Vector_Machine TODO.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m cv \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     pipeline,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     param_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest 5-fold accuracy: \u001b[39m\u001b[39m{\u001b[39;00mgrid\u001b[39m.\u001b[39mbest_score_\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/maxis/Desktop/Bachelor%20Arbeit/bachelor_thesis_dann_fake_news/Support_Vector_Machine%20TODO.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest params:\u001b[39m\u001b[39m\"\u001b[39m, grid\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m   1026\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    971\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    972\u001b[0m         clone(base_estimator),\n\u001b[0;32m    973\u001b[0m         X,\n\u001b[0;32m    974\u001b[0m         y,\n\u001b[0;32m    975\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    976\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    977\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    978\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    979\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    980\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    981\u001b[0m     )\n\u001b[0;32m    982\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    983\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params),\n\u001b[0;32m    984\u001b[0m         \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrouted_params\u001b[39m.\u001b[39;49msplitter\u001b[39m.\u001b[39;49msplit)),\n\u001b[0;32m    985\u001b[0m     )\n\u001b[0;32m    986\u001b[0m )\n\u001b[0;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\joblib\\parallel.py:2071\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2065\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[39m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 2071\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\joblib\\parallel.py:1681\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1681\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1683\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maxis\\miniconda3\\envs\\fake-news-detection\\lib\\site-packages\\joblib\\parallel.py:1799\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1789\u001b[0m     \u001b[39m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m     \u001b[39m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1794\u001b[0m     \u001b[39m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m     \u001b[39m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[39mif\u001b[39;00m (nb_jobs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   1797\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[1;32m-> 1799\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1800\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1802\u001b[0m \u001b[39melif\u001b[39;00m nb_jobs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1803\u001b[0m     \u001b[39m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1804\u001b[0m     \u001b[39m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1810\u001b[0m     \u001b[39m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m     \u001b[39m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.7,\n",
    "        sublinear_tf=True,\n",
    "        stop_words=\"english\",\n",
    "        norm=\"l2\",\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\")),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2), (1, 3)],\n",
    "    \"tfidf__min_df\": [1, 2, 5],\n",
    "    \"tfidf__max_df\": [0.7, 0.85, 0.95],\n",
    "    \"clf__C\": [0.1, 0.5, 1, 2, 5, 10],\n",
    "    \"clf__loss\": [\"hinge\", \"squared_hinge\"],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(f\"Best 5-fold accuracy: {grid.best_score_:.4f}\")\n",
    "print(\"Best params:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Hold-out accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
