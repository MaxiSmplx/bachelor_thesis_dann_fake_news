# Root directory for all raw datasets
datasets_root: "../datasets"

# Flags for text preprocessing steps
preprocessing:
  lowercase: true
  remove_punctuation: true
  remove_urls: true
  remove_digits: true
  strip_html: true
  remove_special_chars: true
  normalize_whitespace: true

# Domain tagging settings
domain_tagging:
  save_embeddings: true
  use_saved_embeddings: true
  n_domains: 13
  plot: true

# Configuration of data augmentation techniques 
# augmentation_budget: Number of requests to send to openai api, avg. cost per 1000 requests: 0.18$ (GPT-4.1 Nano)
augmentation:
  style_transfer:
    enabled: false
  paraphrasing:
    enabled: false
  augmentation_budget: 10

# Settings for text tokenization
tokenization:
  tokenizer_model: "sentence-transformers/all-MiniLM-L6-v2"
  sequence_max_length: 256

# Settings fo balanced output (lower data volume)
balance_data: 
  balance: true
  balance_tolerance: 0.5

# Dataset is split into Train/Validation and Test, this specifies the testsize in %
test_size: 0.15

# Directory for saving processed data
output: "output" 