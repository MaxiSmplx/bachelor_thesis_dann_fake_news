{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_dataset()\n",
      "read_all_datasets()\n",
      "combine_text_columns()\n",
      "split_data()\n",
      "extract_data()\n",
      "evaluate_model()\n",
      "detect_missing_values()\n"
     ]
    }
   ],
   "source": [
    "%run ../common_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing.text_cleaning import (\n",
    "    remove_html,\n",
    "    remove_urls,\n",
    "    remove_digits,\n",
    "    remove_punctuation,\n",
    "    remove_special_characters,\n",
    "    normalize_whitespace,\n",
    "    to_lowercase\n",
    ")\n",
    "from data_preprocessing.tokenizer import tokenize_text\n",
    "from data_preprocessing.feature_extraction import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_augmentation.synonym_replacement import replace_synonyms_database, replace_synonyms_llm\n",
    "from data_augmentation.paraphrasing import paraphrase_llm\n",
    "from data_augmentation.style_transfer import transfer_style_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_all_datasets()\n",
    "\n",
    "data = data.head(10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"augmentation\"][\"synonym_replacement\"][\"enabled\"]:\n",
    "    n_samples = int(max(config[\"augmentation\"][\"synonym_replacement\"][\"n_samples\"] * len(data), 1))\n",
    "    n_replacements = 2\n",
    "\n",
    "    sampled = data.sample(n=n_samples, random_state=42).copy()\n",
    "    augmented_texts = []\n",
    "\n",
    "    if config[\"augmentation_methods\"][\"local_database\"]:\n",
    "        for text in tqdm(sampled['text'], total=n_samples):\n",
    "            augmented_texts.append(replace_synonyms_database(text, n_replacements))\n",
    "    elif config[\"augmentation_methods\"][\"llm\"]:\n",
    "        for text in tqdm(sampled['text'], total=n_samples):\n",
    "            augmented_texts.append(replace_synonyms_llm(text, n_replacements))\n",
    "    \n",
    "    if len(augmented_texts):\n",
    "        sampled['text'] = augmented_texts\n",
    "\n",
    "        data = pd.concat([data, sampled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"augmentation\"][\"paraphrasing\"][\"enabled\"]:\n",
    "    n_samples = int(max(config[\"augmentation\"][\"paraphrasing\"][\"n_samples\"] * len(data), 1))\n",
    "\n",
    "    sampled = data.sample(n=n_samples, random_state=42).copy()\n",
    "    augmented_texts = []\n",
    "\n",
    "    if config[\"augmentation_methods\"][\"llm\"]:\n",
    "        for text in tqdm(sampled['text'], total=n_samples):\n",
    "            augmented_texts.append(paraphrase_llm(text))\n",
    "    \n",
    "        sampled['text'] = augmented_texts\n",
    "\n",
    "        data = pd.concat([data, sampled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"augmentation\"][\"style_transfer\"][\"enabled\"]:\n",
    "    n_samples = int(max(config[\"augmentation\"][\"paraphrasing\"][\"n_samples\"] * len(data), 1))\n",
    "    target_styles = [\n",
    "        \"Neutral journalistic\",\n",
    "        \"Slightly formal\",\n",
    "        \"Slightly informal\",\n",
    "        \"Objective report\",\n",
    "        \"Simplified for general audience\",\n",
    "        \"Summarized headline-style\",\n",
    "        \"Explanatory tone\",\n",
    "        \"Bullet-point format\",\n",
    "        \"Skeptical tone\",\n",
    "        \"Curious/inquisitive tone\"]\n",
    "\n",
    "    sampled = data.sample(n=n_samples, random_state=42).copy()\n",
    "    augmented_texts = []\n",
    "\n",
    "    if config[\"augmentation_methods\"][\"llm\"]:\n",
    "        for text in tqdm(sampled['text'], total=n_samples):\n",
    "            augmented_texts.append(transfer_style_llm(text, choice(target_styles)))\n",
    "    \n",
    "        sampled['text'] = augmented_texts\n",
    "\n",
    "        data = pd.concat([data, sampled], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, config):\n",
    "    if config.get(\"strip_html\"):\n",
    "        text = remove_html(text)\n",
    "    if config.get(\"remove_urls\"):\n",
    "        text = remove_urls(text)\n",
    "    if config.get(\"remove_digits\"):\n",
    "        text = remove_digits(text)\n",
    "    if config.get(\"remove_punctuation\"):\n",
    "        text = remove_punctuation(text)\n",
    "    if config.get(\"remove_special_chars\"):\n",
    "        text = remove_special_characters(text)\n",
    "    if config.get(\"lowercase\"):\n",
    "        text = to_lowercase(text)\n",
    "    if config.get(\"normalize_whitespace\"):\n",
    "        text = normalize_whitespace(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x: clean_text(x, config[\"preprocessing\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 10000/10000 [00:12<00:00, 777.97it/s] \n"
     ]
    }
   ],
   "source": [
    "data_tokenized = tokenize_text(data, tokenizer_name=config[\"tokenization\"][\"tokenizer_model\"], max_length=config[\"tokenization\"][\"sequence_max_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 313/313 [09:49<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "X = extract_features(data_tokenized, model_name=config[\"tokenization\"][\"tokenizer_model\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"label\"].to_numpy(dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = any([config[\"augmentation\"][\"synonym_replacement\"][\"enabled\"],\n",
    "                config[\"augmentation\"][\"paraphrasing\"][\"enabled\"],\n",
    "                config[\"augmentation\"][\"style_transfer\"][\"enabled\"]])\n",
    "\n",
    "np.savez_compressed(f\"output/preprocessed_data{'_augmented' if augmented else ''}.npz\", X=X, y=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
