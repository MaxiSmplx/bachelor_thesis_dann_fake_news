# Fake News Detection Using Domain-Adversarial Neural Networks

**Bachelor Thesis by Maximilian Schulz**  
University of Hamburg â€“ Faculty of Mathematics, Informatics and Natural Sciences  
ğŸ—“ï¸ *May 2025*

---

## ğŸ“˜ Overview

This project investigates the development of a **Domain-Adversarial Neural Network (DANN)** for **robust fake news detection** across multiple domains. The central aim is to enhance the model's **semantic generalization** ability, enabling it to identify fake news not only in a specific dataset or domain but also across diverse platforms (e.g., news articles, social media posts, journalistic statements).

The architecture incorporates **Multi-Domain Learning** and **Adversarial Invariance Techniques**, with extensive comparisons to other machine learning and deep learning baselinesâ€”including **Support Vector Machines, Naive Bayes, Random Forests**, and **Large Language Models**.

---

## â“ Research Question

> _"What training strategies and regularization methods enable the development of a Domain-Adversarial Neural Network for Fake News detection that generalizes across domains and prioritizes semantic robustness over stylistic correlations?"_

---

## ğŸ§  Core Contributions

- Implementation of a **Domain-Adversarial Neural Network (DANN)** with a Gradient Reversal Layer (GRL)
- Benchmarking against **SVM, Naive Bayes, Logistic Regression, CNN**, and **LLMs (Transformer-based models)**
- Integration of **domain-specific data** from various public datasets
- Usage of **data augmentation** and **robustness techniques** (dropout, adversarial training, noise injection)
- Performance evaluation across metrics: Accuracy, Precision, Recall, F1-score, False Positive Rate, and more

---

## ğŸ“Š Datasets Used

### ğŸš‚ Training Datasets

| Dataset | Domain | Type | Size |
|--------|--------|------|------|
| Climate-FEVER | Climate Change | Claims | 1.535 |
| Fake News Corpus | General News | News Articles | 70.846 |
| Fake News Prediction | Politics | News Articles | 6.335 |
| Fakeddit | Mixed | Claims | 7.000 |
| FEVER | Mixed | Claims | 109.810 |
| FineFake | General News | News Articles | 16.909 |
| ISOT Fake News | Politics | News Articles | 44.898 |
| LIAR2 | Politics | Claims | 22.962 |
| llm-misinformation | General News | (LLM generated) News Articles | 7.057 |
| Source based FN | General News | News Articles | 2.050 |
| WELFake | General News | News Articles | 72.095 |


### ğŸ§ª Test Datasets

| Dataset | Domain | Type | Size |
|--------|--------|------|------|

TODO

All datasets can be found at:  
ğŸ”— **[Raw Datasets](https://drive.google.com/drive/folders/1d_2XZ3N9c1Nmncj1CSrnQAaUaoG_xkyv?usp=share_link)** \
ğŸ”— **[Processed Datasets](https://drive.google.com/drive/folders/1-uSCjx6wC7Rh-6gJ1MC0rUFKoF1chw42?usp=share_link)** \
ğŸ”— **[Final Data](https://drive.google.com/drive/folders/1EZ1ci31e1I4LV1axNKYbe0fzqdbNky51?usp=sharing)**


---
## ğŸ“¦ Get the Data

Retrieving data can be done in **4 different ways**, depending on how much control you want over the data acquisition process:

### ğŸ—‚ï¸ Use the Preprocessed All-in-One File

Use the fully processed, combined dataset available in the `/pipeline` directory.  
- The file has been either processed directly or additionally augmented before processing.
- Load them using: `read_processed_data()` (from `common_functions.py`)
- Downloadable via `download_processed_data.py` in `/pipeline`.
- This is the quickest and easiest option â€” no setup required.



### ğŸ§© Use the Individual Processed Files

Available in the `/datasets` directory: `real.parquet` and `fake.parquet`.

- Slightly preprocessed data, split into real and fake news.
- Generated by running `process_dataset.ipynb` on the raw data.
- Load them using:
  - `read_dataset()` for individual files (from `common_functions.py`)
  - `read_all_datasets()` to load all at once (from `common_functions.py`)
- Downloadable via `download_datasets.py` in `/datasets`.



### ğŸ“ Use the Raw Parquet Datasets

Located in `/prepare_datasets` â€” mostly raw `.parquet` files converted from original sources.

- Created by executing `make_dataset.ipynb`.
- Requires further processing using `process_dataset.ipynb`.
- Downloadable via `download_raw_datasets.py` in `/prepare_datasets`.



### ğŸŒ Download from Original Sources

Get the original datasets directly from the official sources listed above.

- After downloading, run `make_dataset.ipynb` and then `process_dataset.ipynb` for full preparation.
- Furthermore, this data should run through the processing and augmentation pipeline found in the pipeline folder


---

## âš™ï¸ Key Methods

- **Domain-Adversarial Neural Network** with:
  - Feature Extractor
  - Label Predictor
  - Domain Classifier
  - Gradient Reversal Layer
- **Text Preprocessing**:
  - Cleaning, tokenization, normalization
  - Feature extraction via Transformers (e.g., BERT)
- **Data Augmentation**:
  - Synonym replacement
  - LLM-based paraphrasing and style transfer
- **Regularization Techniques**:
  - Dropout, Weight Decay, Adversarial Training, Noise Injection

---

## ğŸ“ˆ Evaluation Metrics

- Accuracy
- Precision, Recall, F1-Score
- False Positive / Negative Rate
- Specificity
- Statistical Significance (p-values, confidence intervals)

---

## ğŸ”¬ Baseline Models

- **Traditional ML**: SVM, Naive Bayes, Random Forest
- **Deep Learning**: CNN
- **Transformer-based LLMs**

---

## ğŸ“¦ Environment Setup

All dependencies are listed in the `environment.yml` file.

Create a new conda environment using:

`conda env create -f environment.yml` \
`conda activate fake-news-detection`





