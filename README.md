# Fake News Detection Using Domain-Adversarial Neural Networks

# ğŸ“ Bachelor Thesis

**Lightweight and Generalizable: Domain-Adversarial Neural Networks for Inter-Topic Fake News Detection**  

*Bachelor Thesis by Maximilian Schulz*  
University of Hamburg â€“ Faculty of Mathematics, Informatics and Natural Sciences  
ğŸ—“ï¸ August 2025

---

## ğŸ“˜ Overview

This project investigates the development of a **Domain-Adversarial Neural Network (DANN)** for **robust fake news detection** across multiple domains. The central aim is to enhance the model's **semantic generalization** ability, enabling it to identify fake news not only in a specific dataset or domain but also across diverse topics (e.g., healthcare, politics and entertainment).

The architecture incorporates **Multi-Domain Learning** and **Adversarial Invariance Techniques**, with extensive comparisons to other machine learning and deep learning baselinesâ€”including **Support Vector Machines, Logistic Regression**, and **Large Language Models**.

---

## â“ Research Question

> _"To what extent do Domain-Adversarial Neural Networks outperform traditional and large language models in achieving cross-domain generalization and computational efficiency for fake news detection?"_

---

## ğŸ§  Core Contributions

- Implementation of a **Domain-Adversarial Neural Network (DANN)** with a Gradient Reversal Layer (GRL)
- Benchmarking against **SVM, Logistic Regression, kNN**, and **LLMs (Transformer-based models)**
- Integration of **domain-specific data** from various public datasets
- Usage of **data augmentation** and **data balancing**
- Performance evaluation across metrics: Accuracy, Precision, Recall, F1-score and computational efficiency

---

## ğŸ“Š Datasets

### ğŸš‚ Dataset Overview

| Dataset | Domain | Type | Size |
|--------|--------|------|------|
| Climate-FEVER | Climate Change | Claims | 1,535 |
| Fake News Corpus | General News | News Articles | 9,408,908 |
| FakeNewsNet | Politics & Celebrities | News Articles | 23,196 |
| Fakeddit | Mixed | Claims | 878,218 |
| FEVER | Mixed | Claims | 145,449 |
| FineFake | General News | News Articles | 16.909 |
| MultiFC | Mixed | Claims | 21.148 |
| LIAR2 | Politics | Claims | 22.962 |
| llm-misinformation | General News | (LLM generated) News Articles | 33,383 |
| Source based FN | General News | News Articles | 2,096 |
| WELFake | General News | News Articles | 72,134 |


### â¬‡ï¸ Dataset Download
All datasets can be found at:  
ğŸ”— **[Raw Datasets](https://drive.google.com/drive/folders/1d_2XZ3N9c1Nmncj1CSrnQAaUaoG_xkyv?usp=share_link)** \
ğŸ”— **[Processed Datasets](https://drive.google.com/drive/folders/1-uSCjx6wC7Rh-6gJ1MC0rUFKoF1chw42?usp=share_link)** \
ğŸ”— **[Final Data](https://drive.google.com/drive/folders/1EZ1ci31e1I4LV1axNKYbe0fzqdbNky51?usp=sharing)**


---
## ğŸ“¦ Get the Data

Retrieving data can be done in **4 different ways**, depending on how much control you want over the data acquisition process:

### ğŸ—‚ï¸ Use the Preprocessed All-in-One File

Use the fully processed, combined dataset available in the `/pipeline` directory.  
- The file has been either processed directly or additionally augmented before processing.
- Load them using: `read_processed_data()` (from `common_functions.py`)
- Downloadable via `download_processed_data.py` in `/pipeline`.
- This is the quickest and easiest option â€” no setup required.



### ğŸ§© Use the Individual Processed Files

Available in the `/datasets` directory: `real.parquet` and `fake.parquet`.

- Slightly preprocessed data, split into real and fake news.
- Generated by running `process_dataset.ipynb` on the raw data.
- Load them using:
  - `read_dataset()` for individual files (from `common_functions.py`)
  - `read_all_datasets()` to load all at once (from `common_functions.py`)
- Downloadable via `download_datasets.py` in `/datasets`.



### ğŸ“ Use the Raw Parquet Datasets

Located in `/prepare_datasets` â€” mostly raw `.parquet` files converted from original sources.

- Created by executing `make_dataset.ipynb`.
- Requires further processing using `process_dataset.ipynb`.
- Downloadable via `download_raw_datasets.py` in `/prepare_datasets`.



### ğŸŒ Download from Original Sources

Get the original datasets directly from the official sources listed above.

- After downloading, run `make_dataset.ipynb` and then `process_dataset.ipynb` for full preparation.
- Furthermore, this data should run through the processing and augmentation pipeline found in the pipeline folder


---

## âš™ï¸ Key Methods

- **Domain-Adversarial Neural Network** with:
  - Feature Extractor
  - Label Predictor
  - Domain Classifier
  - Gradient Reversal Layer
- **Text Preprocessing**:
  - Cleaning, tokenization, normalization
  - Feature extraction via Transformers (BERT)
- **Data Augmentation**:
  - LLM-based paraphrasing and style transfer
- **Regularization Techniques**:
  - Dropout, Weight Decay, Regularization, Early Stopping, Lambda Scheduling

---

## ğŸ“ˆ Evaluation Metrics

- Accuracy
- Precision
- Recall
- F1-Score

---

## ğŸ”¬ Baseline Models

- **Traditional ML**: SVM, Logistic Regression, kNN
- **Large Language Models**: BERT, RoBERTa

---

## ğŸ“¦ Environment Setup

All dependencies are listed in the `environment.yml` file.

Create a new conda environment using:

`conda env create -f environment.yml` \
`conda activate fake-news-detection`





